\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage{graphicx}
\usepackage[hyphens]{url}
\usepackage{enumerate}
\usepackage[noadjust]{cite}
\usepackage[pdftex,colorlinks=true]{hyperref}


% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%\bstctlcite{IEEEexample:BSTcontrol}

% paper title
\title{A Workflow for Sustainable Research Software}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing \& Information Systems\\
Cardiff Metropolitan University, UK\\
{\url{tcrick@cardiffmet.ac.uk}}}
\and
\IEEEauthorblockN{Benjamin A. Hall}
\IEEEauthorblockA{MRC Cancer Unit\\
University of Cambridge, UK\\
{\url{bh418@mrc-cu.cam.ac.uk}}}
\and
\IEEEauthorblockN{Samin Ishtiaq}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
{\url{samin.ishtiaq@microsoft.com}}}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass


% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle


\begin{abstract}
Reproducibility of computationally-derived scientific discoveries
should be a certainty. As the product of many person-years' worth of
effort, results -- whether disseminated through academic journals,
conferences or exploited through commercial ventures -- should at some
level be expected to be repeatable by other researchers. While this
stance may appear to be obvious and a fundamental tenet of science, a
variety of factors often stand in the way of making it commonplace,
more so with the increasing dependence on complex software
artefacts. Whilst there has been detailed cross-disciplinary
discussions of the various social, cultural and ideological drivers
and (potential) solutions, one factor which has had less traction is
the concept of software sustainability as a \emph{technical}
challenge. Specifically, that the definition of an \emph{unambiguous}
and \emph{measurable} standard of software sustainability (and thus
reproducibility) would offer a significant benefit to the wider
computational science community.

In this paper, we propose a high-level technical specification as a
starting point for a service that enables software sustainability and
reproducibility, presenting cyberinfrastructure and associated
workflow for a service which would enable such a specification to be
verified and validated. Furthermore, we proposed how it could be
adopted and implemented into the submission workflow for an academic
conference or journal. In addition to addressing a pressing need for
the scientific community, we further speculate on the potential
contribution to the wider software development community of services
which automate \emph{de novo} compilation and testing of code from
source. We illustrate our proposed specification and workflow by using
the {\emph{BioModelAnalyzer}} tool as a running example.
\end{abstract}

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries Keywords here... \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break
% and creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle

\begin{IEEEkeywords}
Software sustainability, Reproducibility, Artefact evaluation,
Benchmarks, Cyberinfrastructure, Research software, Open science
\end{IEEEkeywords}


\section{Introduction}

This paper aims to stimulate a discussion in the wider computational
science community about the sustainability of its algorithms, models,
tools and benchmarks. There is a significant opportunity for this
(broad and diverse) community --- of whom we are proud members --- to
identify and address the technical and socio-cultural issues
surrounding software sustainability (and thus reproducibility) in both
their specific research domain as well as more broadly for
computational science; a desirable outcome would be a clear
specification to encourage, enable and ultimately enforce
sustainability. Enumerating an explicit standard -- one that would emerge,
develop and evolve -- would have a clear benefit for researchers as
well as the wider community as a whole.

Over the past decade, we have seen a step-change in how science and
engineering is done. Experiments, simulations, models, benchmarks,
even proofs cannot be done without leveraging software and
computation. A 2012 report by the Royal Society summarised that
computational techniques have ``{\emph{moved on from assisting
scientists in doing science, to transforming both how science is done
and what science is done}}''~\cite{rssaaoe:2012}, potentially adding
another ``pillar'' to the scientific
method~\cite{hey:2009,vardi-cacm-2010}. Thus, the reproduction and
replication of reported scientific results is a widely discussed topic
within the scientific
community~\cite{schwab-et-al:2000,barnes:2010,mesirov:2010,morin-et-al:2012,joppa-et-al:2013},
even spawning potentially counter-productive legislation in the
USA~\cite{hr4012:2014}.  Whilst the increasing number of high-profile
retractions of scientific studies, from climate science to bioscience,
has drawn the focus of many commentators, automated systems, which
allow easy reproduction of results, offer the potential to improve the
efficiency of scientific exploration and drive the adoption of new
techniques. Nevertheless, this is a wider socio-technical problem that
pervades the scientific community, with estimates that as much as 50\%
of published studies, even those in top-tier academic journals, cannot
be repeated with the same conclusions by an industrial
lab~\cite{osherovich:2011}. Furthermore, just publishing (linked and
open) scientific data is not enough to ensure the required
reusability~\cite{bechhofer-et-al:2013}. There are numerous
non-technical impediments to making software sustainable, maintainable
and re-useable. The pressure to ``make the discovery'' and publish
quickly disincentivises careful software curation, with only recent
imperatives from funding bodies and governments trying to change this
position. Releasing code prematurely was often seen to give your
competitors an advantage, but we should be shining light into these
``black boxes''~\cite{morin-et-al:2012}. In essence: better software,
better research~\cite{goble:2014}.

We should thus exploit a fundamental advantage of the wider impact of
computer science and computational techniques to research: the unique
ability to share the raw outputs of their work as software and
datafiles. New experiments, simulations, models, benchmarks, even
proofs cannot be done without software. And this software does not
consist of simple hack-together, use-once, throw-away scripts;
scientific software repositories contain thousands, perhaps millions,
of lines of code and they increasingly need to be actively supported
and maintained. More importantly, with reproducibility being a
fundamental tenet of science, they should be sustainable and re-useable. However, if
we closely analyse the scientific literature related to software tools
it often does not appear to be adhering to these
rules~\cite{nature:2011}. How many of them are open and reproducible? How many
explain their experimental methodologies, in particular the basis for
their benchmarking? In particular, can we (re)build the
code~\cite{collberg-et-al:2014}\footnote{In turn, stimulating further
discussions in this space:
\url{http://cs.brown.edu/~sk/Memos/Examining-Reproducibility/}}? We,
the authors, are perhaps as guilty as anyone in the past: we have
published papers~\cite{crick-et-al:2009a,berdine-et-al:2011} with
in-depth benchmarks and promises of code to be released online in the
near future.

However, we recognise that in many cases we are ``preaching to the
converted'' and do not need to sell the idea of software
sustainability and reproducibility to the computational science
research community as if it is a new concept. We are used to writing
papers about our algorithms, models and tools; and reproducing others'
work (or having our own work reproduced) is encapsulated in the
``Benchmark Tables'' and presentation of empirical results that
authors and referees in this community both think are essential to any
paper. While the idea of reproducibility has gained momentum across
the wider academic research community -- from computer science and
engineering~\cite{ledet-et-al:2014,collberg-et-al:2015}, to
ecology~\cite{thiele+grimm:2015},
psychology~\cite{chambers-et-al:2014}, political
science~\cite{king:1995} and the social
sciences~\cite{conte-et-al:2012,hutton+henderson:2015} -- we are also
seeing the wider utility of creating and sharing reusable scientific
software, workflows and web
services~\cite{davidson+freire:2008,crick-et-al:2009b,goble:2014,oabarriaga-et-al:2014},
with the emergence of a range of online platforms to support sharing
of data and other research artefacts, including figshare, Open Science
Framework and GitHub. While there has been a revolution in the sharing
and dissemination of published papers (\emph{open access}) and the
subsequent discussions relating to the sharing of protocols and
materials (\emph{open
science})~\cite{pantonprinciples:2010,force11:2011,rssaaoe:2012}, the
ability of a researcher to {\emph{(a)}} keep track of the increasing
number of research outputs in their domain~\cite{parolo-et-al:2015}
and {\emph{(b)}} take these published results and data and reimplement
the described workflow remains
difficult~\cite{peng:2011,koop-et-al:2011,sandve-et-al:2013,wilson-et-al:2014}.

% \footnote{\url{http://figshare.com/}}
% \footnote{\url{https://osf.io/}}
% \footnote{\url{https://github.com/}}


However, there has been promising work in this area, particularly
around benchmarking and
cyberinfrastructure~\cite{sim-et-al:2003,chirigati-et-al:2013,stodden+miguez:2014,stodden-et-al:2015},
along with a number of manifestos and community initiatives to
encourage and support reproducible research, such as the Recomputation
Manifesto~\cite{gent:2013}\footnote{\url{http://www.recomputation.org/}}
and cTuning~\cite{fursin-et-al:2014}, as well as curated
recommendations on where to publish research
software\footnote{\url{http://www.software.ac.uk/resources/guides/which-journals-should-i-publish-my-software}}. We
have also seen the emergence of successful initiatives, such as the
Software Sustainability
Institute\footnote{\url{http://www.software.ac.uk/}}, Software
Carpentry\footnote{\url{http://software-carpentry.org/}} and the UK
Community of Research Software
Engineers\footnote{\url{http://www.rse.ac.uk}}, in cultivating
world-class research through software, developing software skills and
raising the profile of research software engineers. Furthermore, we
have previously encapsulated some of the technical barriers to
reproducing work across computing and the computational sciences,
particular in terms of the sharing of algorithms, models and benchmark
sets~\cite{crick-et-al_wssspe2,crick-et-al_recomp2014,crick-et-al:2015},
as well as drawing attention to some of the wider socio-cultural
issues~\cite{chuehong-et-al:2015}.

While reproducibility is not a new concept to the computer science
research community~\cite{price:1986}, more recently a number of
high-profile computer science conferences, including PLDI, POPL,
SIGMOD, CGO, SPLASH and ECAI, have implicitly acknowledged the
importance of software sustainability via artefact evaluation and
reproducibility\footnote{See \url{http://www.artifact-eval.org/} and
\url{http://ctuning.org/reproducibility}} (and repeatability,
recomputability and the multitude of `Rs' that underpin
e-research~\cite{deroure:2010,feitelson:2015}), as well as promoting
community-driven reviewing and validation~\cite{fursin+dubach:2014}.
For many, this takes the form of the author providing \emph{artefacts}
--- an accessible tool for reproducing results --- for the reviewers
to evaluate. Journals such as Science, Nature, PLoS and from Royal
Society Publishing explicitly require that source code and data is
made available online under some form of open source license. While
these initiatives are laudable, they are often optional, seem
piecemeal, and do little to easily enable verification or validation
of scientific results at a later stage. Even within the same field,
there are different ideas of what defines reproducibility.

In contrast to more traditional research outputs, the development of
algorithms has unique advantages. Algorithms are tested through their
implementation in code, and as such they can be accurately
communicated either by sharing the implementation and a pseudo-code
description of the algorithm behaviour. Furthermore, because of their
discrete nature it is expected that they always return the same output
for the same input. It follows that these features are more easily
tested than the outputs of other scientific disciplines. To
demonstrate the reproducibility of a newly developed algorithm, one
must {\emph{(a)}} be able to create an operational artefact from the
code, and {\emph{(b)}} show that published models (i.e. benchmarks)
show the same behaviours as those reported. This specification for
reproducibility can in principle be tested for every publication in
the field.

Therefore, this paper invites computational researchers and research
software engineers to help develop and curate a new methodology (and
culture) for disseminating software-dependent research. We propose an
initial specification and high-level requirements for a service, along
with a suggested plan for introducing it to key conferences and
journals in a field. In discussing the service, we highlight key
implementation issues relating to security and general applicability
which will need mitigating or resolving before widespread acceptance
by a specific research community.  The benefits of such are service
should be clear. In a sense, software sustainability and
reproducibility here is a coming together of the standard practice of
\emph{testing} and ongoing work done in many continuous integration
systems (notably, automated build services). Three features are
prominent in our aims for promoting sustainable software engineering
processes: compilation and testing in a new machine, a continuous
integration strategy for code commits and the ability to add and
remove benchmarks to the test set.

\section{A Specification for Sustainable Research Software Engineering}\label{spec}

A service for sustainable research software engineering should
explicitly enable reproducibility and is intended to play three important
roles; it should:

\begin{enumerate}[(i)]
\item Demonstrate that the source of an algorithm or tool can be
  compiled, run and behave as described, without manual intervention
  from the author; 
\item Allow new benchmarks to be added, by users other than
the developer, to widen the testing and identify potential bugs; 
\item Store and link specific artefacts with their linked
publications or other publicly-accessible datasets. 
\end{enumerate}

We use a running example based upon
{\emph{BioModelAnalyzer}}\footnote{Available online:
\url{http://biomodelanalyzer.research.microsoft.com/}}, a tool for the
development and analysis (simulation, model checking) of a specific
class of formal models for biology, which has been described by the
authors over a series of VMCAI and CAV
papers~\cite{cook-et-al:2011,benque-et-al:2012,cook-et-al:2014}.  The
tool specifically allows users to test for model \emph{stability};
that is, a bespoke algorithm proves that for all initial states a
model always ends in a single, unique fixpoint. We have chosen this
example due to our familiarity with the tool, and to highlight
historical examples where a software sustainability and
reproducibility service would have supported both toolchain
development and algorithm discovery.

\subsection{There are Two Types of People}

\begin{figure*}[!htp]
	\centering
	\includegraphics[width=\textwidth]{images/workflow}
	\caption{Proposed service workflow}
	\label{schematic}
\end{figure*}

To address these needs, we propose that the service should follow the
workflow presented in Figure~\ref{schematic}. Two main classes of user
are initially defined: developers, who generate code; and modellers,
who generate new benchmarks. An individual might in practice play
either or both of these roles; here the roles serve to define ways in
which people interact with the system. A developer writes new code,
which is periodically pushed to a public repository such as
GitHub. Through integration with the repository, the server responds
to new code by undergoing a process of pulling the code from the
repository, downloading required dependencies, and compiling the
code. If this stage fails, the developer is informed and the workflow
ends. If the code is successfully compiled, two stages of testing are
performed.

The first stage (labelled {\emph{Test}} in Figure~\ref{schematic}),
involves running a series of basic tests defined by the
developer. This is intended as a sanity check to ensure that basic
features of the code have not been broken by the updated code, and
failure to pass these tests is reported and ends the workflow. If this
completes successfully, the second stage (labelled {\emph{Benchmark}}
in Figure~\ref{schematic}) runs and a series of models are tested for
a known property, and the results recorded. These results can then be
stored in a database, with a note of the commit ID, and available
through a web interface for future audit and analysis.

The service must fit easily into the developer
workflow~\cite{venters-et-al:2014}; as noted in Section~\ref{rollout}
we expect that there will naturally be some costs to the users in
terms of the time required to ensure that the code compiles and runs
on the service. To minimise this, the service needs to easy connect to
common code repositories, automatically detecting and responding to
new versions of the code and updates to dependencies, running tests
for every new code commit.

\subsection{{\emph{de Novo}} Build Environments}

To begin to address the socio-cultural issues discussed earlier, a
service such as this must require minimal developer intervention.
This serves multiple purposes -- through automation for example, the
service can be enabled to compile new code and test new benchmarks
trivially. This also forces the developer to make publicly available
their local workarounds (i.e. hacks and workflow ``glue''). As such,
this requires the developer to make the project dependencies clearly
available, and enables future changes in the dependencies (such as a
library update) to be tested automatically too.

Throughout the lifetime of the {\emph{BioModelAnalyzer}} tool,
development has been shared between a number of developers, each
working on different aspects of the tool. Work in algorithm
development focuses on adding new features to a command line tool with
few dependencies, aiding rapid development. In contrast, the graphical
model construction and testing environment has typically been done by
a single or pair of individuals.  This necessarily required a number
of dependencies, reflecting the use of Azure (Microsoft's cloud
computing platform) and Silverlight (a framework for rich Internet
applications).

In an early stage of development it was found that only a single
machine was capable of deploying the web service. This arose as the
developer responsible for writing and deploying the user interface had
run a series of commands necessary to run the mixture of 32- and 64-
bit components on Azure. These commands needed only be run once, and
went undocumented, thus needing to be rediscovered later when other
team members attempted to deploy. These problems would be identified
trivially through the proposed service; such undocumented commands
would lead to all tests failing until explicitly added to the build
process.

\subsection{Tool Refinement}

In contrast to the developer role, a modeller supplies benchmarks for
a piece of code to test against. These do not require that the latest
version of the code is recompiled, but on submission the models are
tested and added to the local repository of models for analysis.

In the case of {\emph{BioModelAnalyzer}}, throughout the development of the
tool, many refinements have been made to different
implementations. Some of these were subtle, and were identified by
unit tests; for example rounding mechanisms were switched between
floors and rounds following a scientific discussion. More complex
changes however broke behaviours which were not tested in our
available benchmark set. One example was in the treatments of nodes
without inputs; ``biological intuition'' suggested that such nodes
should have an alternative default function from other nodes. Here,
the ability of users to submit new benchmarks would aid identification
of these breaking changes, by extending the test sets and simplifying
the process of adding to the test sets, and forcing the question of
what changes are appropriate (and how to update old models to keep
correct behaviour).


\subsection{Identification of Algorithmic Weaknesses}

After a model is submitted, it is tested on every new piece of code
pushed to the server and the changes in the behaviour can be noted and
linked to specific code commits. Whilst the developer's role has a
transparent value (in providing an implementation of an algorithm),
the value of the modeller may be less immediately clear. The modeller
submits a broad range of tests which may highlight material flaws
(i.e. bugs) in the implementation, or the algorithm. More than this
however, the modeller may generate models which identify weaknesses of
either an algorithm or a specific implementation.

One example from the authors experience is the series of models with
``timed-switches'' described in~\cite{cook-et-al:2014}. There, we
presented a new algorithm for proving stability in a new class of
models. Whilst the paper focused on discussing the algorithm,
identifying the new class of models was complex. Models with long
cycles and the new class of models (``non-trivially stable models'')
both can take substantial time to search for cycles, and these models
could only be proved stable using a combination of simulation (to
identify the fixpoint) and LTL queries (to prove that there existed
no paths beyond a certain length which did not include the
fixpoint)~\cite{claessen-et-al:2013}.

\subsection{Algorithm-Model Axes}

The proposed service would allow both algorithm and model type of
tests to be included explicitly, and models to be routinely added to
each algorithm. Models which time-out with one but are successfully
proved can be logged and identified for future study. The features
which define them could then be more easily researched, and new algorithms
developed to address the specific features of the model. It could
further be used to demonstrate the speed improvement arising from new
algorithms.

\subsection{{\texttt{make depend}}}

Dependencies for a given implementation need explicit testing. Due to
the highly variable and sometimes complex nature of dependencies, we
see this as an optional part of the workflow, as developers may chose
to supply certain dependencies as binary files in the code compilation
process. For completeness however, we note that such a system could
also respond to updates in external dependencies by triggering
compilation and testing in the same manner as defined for a new code
commit. This would aid developers in identifying code breaking changes
introduced by third parties.

\subsection{``{\emph{I'm First!}}''}

Another issue is around performance comparisons of benchmarks: how can
we estimate, compare and evaluate raw performance in the cloud?
Testing new algorithms on benchmarks is in the first instance about
pass/fail, but very soon the focus is on raw performance. Benchmark
tables are about out-performing other algorithms, other tools. But, if
the whole verification workflow is running on the cloud, then
acquiring and evaluating raw performance numbers is not immediately
feasible. There is no cloud equivalent of {\texttt{top}} or
{\texttt{time}} that gives user--resource statistics. There is too
much infrastructure interference/dependence --- with VMs spinning up,
being torn down, migrating, the bus being used by other VMs, etc ---
to obtain easily comparable numbers for the user/process/VM
itself. Projects such as
Recomputation.org\footnote{\url{http://recomputation.org/}} have been
focusing on using virtual machines in the cloud to freeze, and later
unfreeze, computational experiments; while this approach is not the
complete solution, it is certainly a move in the right
direction~\cite{arabas-et-al:2014}. Nevertheless, the project's
primary aim is to validate recomputation~\cite{gent:2013}, with
performance a secondary consideration~\cite{gent+kotthoff:2014}, thus
making this a key avenue for further investigation. Nevertheless, we
do not overlook or neglect performance modelling as a key metric for
certain researchers, as this is clearly important. While it is
non-trivial to accurately assess the time required to solve problems
in the cloud, it is certainly possible to construct cloud
computational models and assess how measured times line up with bounds
derived from the model.


\subsection{Running Arbitrary Code}

There are clearly significant potential security concerns around
providing open cyberinfrastructure that pulls, compiles and runs
arbitrary code as part of an autonomous continuous integration
framework; we need to consider precisely how this infrastructure would
interact with other open services, as well as the privileges it would
require to run as an autonomous cloud service. Nevertheless, there are
existing models of sandboxing and privilege restriction from elastic
cloud computation providers that could be further developed and
applied.


\section{A Sustainable Software Workflow for a Computational Research Community}
\label{rollout}

Following the proposal of such a system, the question becomes:
{\emph{how do we encourage widespread uptake, or even
    standardisation?}}  Such a service may appear non-trivial, given
the large numbers of tools and workflows that could potentially
require to be supported by the service. Furthermore, after such a
service has been implemented, how do we ensure it is \emph{useful} and
\emph{usable} for researchers. To address this, we propose the
following workflow that could be adapted and used for conferences in computational
science community:

% workflow
\begin{enumerate}
\item {\textbf{Pre-conference:}} clear signposting for authors; it
should be advertised and promoted in the call for papers to
highlight this is a step-change in how we address
reproducibility. Call for artefact reviewers with a range of
specialisms, with a named chair of the review team.
\item {\textbf{Explicit criteria for authors:}} {\emph{make this as
easy as possible for us to evaluate/execute your artefact!}}. We would
aim to articulate the review criteria, but the primary aim is:
{\emph{can I evaluate/execute this artefact and get the same results
that are presented in the paper?}}
\item {\textbf{Submission:}} when papers are submitted, they have to
nominate whether they want their paper to go through artefact review
(at the start, this may not be compulsory, but this will change over a
period of time -- effecting cultural change and this would then become
a necessary condition and a formal stage in the reviewing workflow),
along with required tools, libraries and (ideally) computational
requirements.
\item {\textbf{Reviewing:}} in the first instance, it may be seen as
an extra (voluntary) step to the normal reviewing process:
e.g. {\emph{This submission is voluntary and will not influence the
final decision regarding the papers.}}. Independent of the scientific
merit of the paper, the results will be verified. To encourage this,
there may be a prize, as well as ranked ordering and profiled listed
in conference proceedings/final publication.
\item {\textbf{Artefact evaluation:}} artefact evaluation process runs
concurrent to the standard paper review process.
\item {\textbf{Reporting:}} traffic lights system (potentially with
ranked list) to indicate the level of reproducibility of the submitted
artefact.
\item {\textbf{Community curation:}} over a number of conference
  cycles, we would have a community curated repository/database of
  previous artefacts, which would provide exemplars, comparisons and
  emerging best practice.
\end{enumerate}

The key question for different research communities then becomes:
{\emph{how to initiate and initialise this change?}} Such a
requirement creates a set of new costs to researchers, both in terms
of time spent ensuring that their tools work on the centralised system
(in addition to their local implementation), but also potentially in
terms of equipment (in terms of running the system).  Such costs may
be easier to bear for some researchers compared to others, especially
those with large research groups who can more easily distribute the
tasks, and it is important that the service does not present a barrier
to early career researchers and those with efficient budgets (this
type of cost analysis is not unique to reproducibility efforts -- it
has been estimated that a shift to becoming exclusively open access
for a journal may lead to a ten-fold increase in computer science
publication costs~\cite{vardi-cacm-2014}). Another advantage of such a
service however is that it sets a clear minimum standard for
reproducibility in computational research. It does not impose a
specific set of licences (a limited licence for testing would be all
that is necessary). By its nature, it can be uniformly applied to all
users in a conference, preventing the ``weaponisation of
reproducibility'' that has been highlighted as a potential
weakness\footnote{\url{http://simplystatistics.org/2015/03/13/de-weaponizing-reproducibility/}}.


\section{Conclusions}\label{concl}

The benefits to the community from a cultural change to foster and
favour reproducibility are clear and as such we should aim through the
development and adoption of open cyberinfrastructure, software
toolchains and workflow to mitigate these costs. Real world
implementation of the idea would require the development of a
standard, and in order to be successful, would require near
simultaneous participation from all major journals and conferences.
Since compliance would add some burden on the authors and the
reviewers, without uniform adoption, journals and conferences adopting
the service would be at a disadvantage. We recognise this is where the
wider socio-cultural levers within academia will clash with the
ongoing push for open research (along with new and evolving forms of
research dissemination, reward and recognition).

However, we can reasonably expect the needs of the community to evolve
over time, and initial implementations of the platform will naturally
require refinement in response to user/community feedback. As such, if
different communities are to move to requiring models and software
sustainability and reproducibility, it seems most reasonable that this
is staggered over a number of years to allow for both of these
elements to develop, until eventually all researchers are required to
use the service. This plan balances competing needs within the
community, and would reduce the disruption for uptake by gradually
introducing it to researchers.

\begin{itemize}
\item {\textbf{Year {\emph{t}}:}} Offer the service as an optional extra in the
  testing phase, allowing users to demonstrate the reliability of
  their code which could be taken into account in the review process.
\item {\textbf{Year {\emph{t+1}}:}} All authors must use the reproducibility
  service, but results are not used in the review process. The results
  of the test are used to refine the service and pick out any
  unaddressed issues
\item {\textbf{Year {\emph{t+2}}:}} All authors are required to use the service, and
  the results are explicitly used to assess reproducibility in the
  review process.
\end{itemize}

This open discussion, understanding and acceptance of what software
sustainability -- and more generally, reproducibility -- means for the
wider computational research community is clearly important. It is
imperative that we see it as worthwhile and address it, but we now
have to move beyond manifestos, pledges and top tips: as researchers,
we all need to publicly acknowledge that this is worthwhile and put
concrete measures in place to address it, or stop going on about it as
being a laudable aspiration but impossible to achieve in practical
terms. We very much look forward to being a part of a community of
researchers developing this idea, culture and infrastructure over the
coming months.

%\newpage

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
\IEEEtriggeratref{50}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
\bibliographystyle{IEEEtran}
\bibliography{ssse15}

% that's all folks
\end{document}


